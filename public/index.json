[
{
	"uri": "//localhost:1313/3-migrate/3.3-k8s-app/3.3.1-alb-ingress-controller/",
	"title": "AWS ALB Ingress Controller",
	"tags": [],
	"description": "",
	"content": "At this time, app-phonebook has been deployed in Private Subnet, so if you need to expose App to the Internet, we need to deploy AWS Load balancer in Public Subnet pointing to App-phonebook in Private Subnet.\nAmazon Application Load Balancer (ALB) is a popular AWS service that load balances incoming traffic at the application layer (layer 7) across multiple targets, such as Amazon EC2 instances, in a region. ALB supports multiple features including host or path based routing, TLS (Transport Layer Security) termination, WebSockets, HTTP/2, AWS WAF (Web Application Firewall) integration, integrated access logs, and health checks.\nKubernetes Ingress is an API resource that allows you manage external or internal HTTP(S) access to Kubernetes services running in a cluster\nThe open source AWS ALB Ingress controller triggers the creation of an ALB and the necessary supporting AWS resources whenever a Kubernetes user declares an Ingress resource in the cluster. The Ingress resource uses the ALB to route HTTP(S) traffic to different endpoints within the cluster. The AWS ALB Ingress controller works on any Kubernetes cluster including Amazon Elastic Kubernetes Service (Amazon EKS).\nHow AWS Load Balancer controller works:\n[1]: The controller watches for ingress events from the API server. When it finds ingress resources that satisfy its requirements, it begins the creation of AWS resources.\n[2]: An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal. You can also specify the subnets it\u0026rsquo;s created in using annotations.\n[3]: Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource.\n[4]: Listeners are created for every port detailed in your ingress resource annotations. When no port is specified, sensible defaults (80 or 443) are used. Certificates may also be attached via annotations.\n[5]: Rules are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service.\nAlong with the above, the controller also\u0026hellip;\ndeletes AWS components when ingress resources are removed from k8s. modifies AWS components when ingress resources change in k8s. assembles a list of existing ingress-related AWS components on start-up, allowing you to recover if the controller were to be restarted. AWS Load Balancer controller supports two traffic modes:\nInstance mode: Ingress traffic starts at the ALB and reaches the Kubernetes nodes through each service\u0026rsquo;s NodePort. This means that services referenced from ingress resources must be exposed by type:NodePort in order to be reached by the ALB. IP mode: Ingress traffic starts at the ALB and reaches the Kubernetes pods directly. CNIs must support directly accessible POD ip via secondary IP addresses on ENI (Ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html) By default, Instance mode is used, users can explicitly select the mode via alb.ingress.kubernetes.io/target-type annotation. Install AWS ALB Ingress Controller:\nStep 1: Create Kubernetes RBAC role \u0026amp; Service Account for ALB Ingress Controller.\nWe will create manifest file to create permission for aws-load-balancer-controller, then apply it:\ncat \u0026lt;\u0026lt;EOF \u0026gt; rbac-role-aws-load-balancer-controller.yaml\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRole\rmetadata:\rlabels:\rapp.kubernetes.io/name: aws-load-balancer-controller\rname: aws-load-balancer-controller\rrules:\r- apiGroups:\r- \u0026quot;\u0026quot;\r- extensions\rresources:\r- configmaps\r- endpoints\r- events\r- ingresses\r- ingresses/status\r- services\r- pods/status\rverbs:\r- create\r- get\r- list\r- update\r- watch\r- patch\r- apiGroups:\r- \u0026quot;\u0026quot;\r- extensions\rresources:\r- nodes\r- pods\r- secrets\r- services\r- namespaces\rverbs:\r- get\r- list\r- watch\r---\rapiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rlabels:\rapp.kubernetes.io/name: aws-load-balancer-controller\rname: aws-load-balancer-controller\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: aws-load-balancer-controller\rsubjects:\r- kind: ServiceAccount\rname: aws-load-balancer-controller\rnamespace: kube-system\r---\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rlabels:\rapp.kubernetes.io/name: aws-load-balancer-controller\rname: aws-load-balancer-controller\rnamespace: kube-system\r...\rEOF\rkubectl apply -f rbac-role-aws-load-balancer-controller.yaml\rStep 2. Create IAM Policy for ALB Ingress Controller\nThis IAM policy will allow our ALB Ingress Controller pod to make calls to AWS APIs\nStep 2.1: Go to IAM Service: https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1\nStep 2.2: Select Policies, then click to Create Policies: Step 2.3: Enter policy for ALB Ingress Controller as bellow:\n{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;acm:DescribeCertificate\u0026quot;,\r\u0026quot;acm:ListCertificates\u0026quot;,\r\u0026quot;acm:GetCertificate\u0026quot;,\r\u0026quot;ec2:*\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;ec2:AuthorizeSecurityGroupIngress\u0026quot;,\r\u0026quot;ec2:CreateSecurityGroup\u0026quot;,\r\u0026quot;ec2:CreateTags\u0026quot;,\r\u0026quot;ec2:DeleteTags\u0026quot;,\r\u0026quot;ec2:DeleteSecurityGroup\u0026quot;,\r\u0026quot;ec2:DescribeAccountAttributes\u0026quot;,\r\u0026quot;ec2:DescribeAddresses\u0026quot;,\r\u0026quot;ec2:DescribeInstances\u0026quot;,\r\u0026quot;ec2:DescribeInstanceStatus\u0026quot;,\r\u0026quot;ec2:DescribeInternetGateways\u0026quot;,\r\u0026quot;ec2:DescribeNetworkInterfaces\u0026quot;,\r\u0026quot;ec2:DescribeSecurityGroups\u0026quot;,\r\u0026quot;ec2:DescribeSubnets\u0026quot;,\r\u0026quot;ec2:DescribeTags\u0026quot;,\r\u0026quot;ec2:DescribeVpcs\u0026quot;,\r\u0026quot;ec2:ModifyInstanceAttribute\u0026quot;,\r\u0026quot;ec2:ModifyNetworkInterfaceAttribute\u0026quot;,\r\u0026quot;ec2:RevokeSecurityGroupIngress\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;elasticloadbalancing:AddListenerCertificates\u0026quot;,\r\u0026quot;elasticloadbalancing:AddTags\u0026quot;,\r\u0026quot;elasticloadbalancing:CreateListener\u0026quot;,\r\u0026quot;elasticloadbalancing:CreateLoadBalancer\u0026quot;,\r\u0026quot;elasticloadbalancing:CreateRule\u0026quot;,\r\u0026quot;elasticloadbalancing:CreateTargetGroup\u0026quot;,\r\u0026quot;elasticloadbalancing:DeleteListener\u0026quot;,\r\u0026quot;elasticloadbalancing:DeleteLoadBalancer\u0026quot;,\r\u0026quot;elasticloadbalancing:DeleteRule\u0026quot;,\r\u0026quot;elasticloadbalancing:DeleteTargetGroup\u0026quot;,\r\u0026quot;elasticloadbalancing:DeregisterTargets\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeListenerCertificates\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeListeners\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeLoadBalancers\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeLoadBalancerAttributes\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeRules\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeSSLPolicies\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeTags\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeTargetGroups\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeTargetGroupAttributes\u0026quot;,\r\u0026quot;elasticloadbalancing:DescribeTargetHealth\u0026quot;,\r\u0026quot;elasticloadbalancing:ModifyListener\u0026quot;,\r\u0026quot;elasticloadbalancing:ModifyLoadBalancerAttributes\u0026quot;,\r\u0026quot;elasticloadbalancing:ModifyRule\u0026quot;,\r\u0026quot;elasticloadbalancing:ModifyTargetGroup\u0026quot;,\r\u0026quot;elasticloadbalancing:ModifyTargetGroupAttributes\u0026quot;,\r\u0026quot;elasticloadbalancing:RegisterTargets\u0026quot;,\r\u0026quot;elasticloadbalancing:RemoveListenerCertificates\u0026quot;,\r\u0026quot;elasticloadbalancing:RemoveTags\u0026quot;,\r\u0026quot;elasticloadbalancing:SetIpAddressType\u0026quot;,\r\u0026quot;elasticloadbalancing:SetSecurityGroups\u0026quot;,\r\u0026quot;elasticloadbalancing:SetSubnets\u0026quot;,\r\u0026quot;elasticloadbalancing:SetWebAcl\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;iam:CreateServiceLinkedRole\u0026quot;,\r\u0026quot;iam:GetServerCertificate\u0026quot;,\r\u0026quot;iam:ListServerCertificates\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;cognito-idp:DescribeUserPoolClient\u0026quot;,\r\u0026quot;eks:*\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;waf-regional:GetWebACLForResource\u0026quot;,\r\u0026quot;waf-regional:GetWebACL\u0026quot;,\r\u0026quot;waf-regional:AssociateWebACL\u0026quot;,\r\u0026quot;waf-regional:DisassociateWebACL\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;tag:GetResources\u0026quot;,\r\u0026quot;tag:TagResources\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;waf:GetWebACL\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;wafv2:GetWebACL\u0026quot;,\r\u0026quot;wafv2:GetWebACLForResource\u0026quot;,\r\u0026quot;wafv2:AssociateWebACL\u0026quot;,\r\u0026quot;wafv2:DisassociateWebACL\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r},\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Action\u0026quot;: [\r\u0026quot;shield:DescribeProtection\u0026quot;,\r\u0026quot;shield:GetSubscriptionState\u0026quot;,\r\u0026quot;shield:DeleteProtection\u0026quot;,\r\u0026quot;shield:CreateProtection\u0026quot;,\r\u0026quot;shield:DescribeSubscription\u0026quot;,\r\u0026quot;shield:ListProtections\u0026quot;\r],\r\u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;\r}\r]\r}\rStep 2.4: Click on Create Policy\nStep 3: Create an IAM role for the ALB Ingress Controller and attach the role to the service account\neksctl create iamserviceaccount \\\r--region us-east-1 \\\r--name aws-load-balancer-controller \\\r--namespace kube-system \\\r--cluster phonebook-eks-cluster \\\r--attach-policy-arn arn:aws:iam::413403005747:policy/ALBIngressControllerIAMPolicy \\\r--override-existing-serviceaccounts \\\r--approve\rStep 4: Install cert-manager\ncert-manager is a Kubernetes addon to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry.\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.12.3/cert-manager.yaml\rStep 5: Deploy ALB Ingress Controller\nDownload the manifest file for the ALB Ingress Controller\nwget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.7.0/v2_7_0_full.yaml\rEdit the saved yaml file, go to the Deployment spec, and set the controller \u0026ndash;cluster-name arg value to your EKS cluster name\napiVersion: apps/v1\rkind: Deployment\r. . .\rname: aws-load-balancer-controller\rnamespace: kube-system\rspec:\r. . .\rtemplate:\rspec:\rcontainers:\r- args:\r- --cluster-name=\u0026lt;your-cluster-name\u0026gt;\rIf you use IAM roles for service accounts, we recommend that you delete the ServiceAccount from the yaml spec. If you delete the installation section from the yaml spec, deleting the ServiceAccount preserves the eksctl created iamserviceaccoun Apply the yaml file:\nkubectl apply -f v2_7_0_full.yaml\rWe have successfully installed AWS ALB Ingress Controller: "
},
{
	"uri": "//localhost:1313/4-backup-restore/4.1-backup/",
	"title": "Backup mysql database trên môi trường On-premsise",
	"tags": [],
	"description": "",
	"content": "Run command to access to MySQL Pod:\nkubectcl exec –it \u0026lt;mysql_pod\u0026gt; bash –n phonebook\rRun command to backup:\nmysqldump -u root -p phonebook \u0026gt; /var/lib/mysql/database_phonebook.sql\rBecause the /var/lib/mysql folder in the mysql container is mounted with the /mnt/data folder on the host, so we can see the database_phonebook.sql file in the /mnt/data folder on the host.\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/3.2.1-vpc-subnet/",
	"title": "Create AWS VPC &amp; Subnets using Terraform",
	"tags": [],
	"description": "",
	"content": "The next step is to create a virtual private cloud in AWS using the \u0026ldquo;aws_vpc\u0026rdquo; resource. There is one required field that need to provide, which is the size of my network.\nThen, I create 2 public subnets and 2 private subnets:\n# Create VPC\rresource \u0026quot;aws_vpc\u0026quot; \u0026quot;vpc-main\u0026quot; {\rcidr_block = \u0026quot;10.10.0.0/16\u0026quot;\renable_dns_hostnames = true\renable_dns_support = true\rtags = {\rName = \u0026quot;vpc-main\u0026quot;\r}\r}\routput \u0026quot;aws_vpc\u0026quot; {\rvalue = aws_vpc.vpc-main.id\r}\r# Create Subnet\rresource \u0026quot;aws_subnet\u0026quot; \u0026quot;public-subnet-vpc-main-1\u0026quot; {\rvpc_id = aws_vpc.vpc-main.id\rcidr_block = \u0026quot;10.10.1.0/24\u0026quot;\ravailability_zone = \u0026quot;us-east-1a\u0026quot;\rmap_public_ip_on_launch = true\rtags = {\rName = \u0026quot;public-subnet-vpc-main-1\u0026quot;\r}\r}\rresource \u0026quot;aws_subnet\u0026quot; \u0026quot;public-subnet-vpc-main-2\u0026quot; {\rvpc_id = aws_vpc.vpc-main.id\rcidr_block = \u0026quot;10.10.2.0/24\u0026quot;\ravailability_zone = \u0026quot;us-east-1b\u0026quot;\rmap_public_ip_on_launch = true\rtags = {\rName = \u0026quot;public-subnet-vpc-main-2\u0026quot;\r}\r}\rresource \u0026quot;aws_subnet\u0026quot; \u0026quot;private-subnet-vpc-main-1\u0026quot; {\rvpc_id = aws_vpc.vpc-main.id\rcidr_block = \u0026quot;10.10.3.0/24\u0026quot;\ravailability_zone = \u0026quot;us-east-1a\u0026quot;\rtags = {\rName = \u0026quot;private-subnet-vpc-main-1\u0026quot;\r}\r}\rresource \u0026quot;aws_subnet\u0026quot; \u0026quot;private-subnet-vpc-main-2\u0026quot; {\rvpc_id = aws_vpc.vpc-main.id\rcidr_block = \u0026quot;10.10.4.0/24\u0026quot;\ravailability_zone = \u0026quot;us-east-1b\u0026quot;\rtags = {\rName = \u0026quot;private-subnet-vpc-main-2\u0026quot;\r}\r}\r"
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/3.1.1-create-ec2-instance/",
	"title": "Create Bastion host",
	"tags": [],
	"description": "",
	"content": "Step 1: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/ -\u0026gt; Choose Launch Instance.\nStep 2: Configure EC2 Instance:\nName and tags: Bastion host, select AMI: EC2 Instance type: t2.small Configure Network: In which: Security Group for bastion allow rule is necessary for the Instance Connect service to be accessible\nStep 3: Choose Review and Launch, then choose Launch\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/",
	"title": "Create Bastion Host and install the necessary tools",
	"tags": [],
	"description": "",
	"content": "Content Create Bastion host Create access key \u0026amp; secret key Setup Instance Connect to access Bastion host via ssh protocol Install awscli tool Install eksctl tool Install kubectl tool Install Terraform tool "
},
{
	"uri": "//localhost:1313/2-architecture/2.1-on-prem/",
	"title": "Current architecture of the Kubernetes system on On-premises environment",
	"tags": [],
	"description": "",
	"content": "Content Kubernetes cluster architecture on On-premises environment The application architecture is deployed using Kubernetes on On-premises environment "
},
{
	"uri": "//localhost:1313/2-architecture/2.1-on-prem/2.1.1-k8s-cluster/",
	"title": "Kubernetes cluster architecture on On-premises environment",
	"tags": [],
	"description": "",
	"content": "\nThis architecture ensures High Availability, so there are 3 Server Control Plane with an External etcd cluster consisting of 3 Servers, 2 Servers are responsible for load balancing according to the active-standby model and Worker nodes. So on the On-premise model, we have to manage and take care of the entire architecture as above and handle issues that occur, always ensuring High Availability and take care of backup \u0026amp; restore scenarios for Infrastructure when an incident occurs\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Overall",
	"tags": [],
	"description": "",
	"content": "Migrate services deployed using Kubernetes from On-premises environment to Cloud environment Overall This Workshop demonstrates Migrating a simple Micro-service phonebook application deployed on a Kubernetes cluster in an On-premises environment to an AWS Cloud environment, as well as building an EKS cluster architecture using Infrastrure as code tool is Terraform\nContent Problem Architecture Migrate service from On-premises environment to AWS cloud environment Backup \u0026amp; restore application data from on-premises enviroment to AWS cloud enviroment Clean up resources "
},
{
	"uri": "//localhost:1313/1-problem/",
	"title": "Problem",
	"tags": [],
	"description": "",
	"content": "XYZ company has deployed their Services using the Kubernetes platform on an On-premises environment. However, after a while, the company realized operational problems, had to manage the infrastructure itself, needed to ensure High Availability for Control Plane, Etcd, etc., so it took a lot of time and human resources.\nIn that context, the DevOps team received a request from The Leadership about the company\u0026rsquo;s special event, when it was predicted that the number of requests to Services would increase 3 times. The DevOps team has checked that it is difficult for current resources to meet the 3-times increase in requests during the company\u0026rsquo;s special event. Therefore, in order to solve the problem of rapid and cost-effective expansion, the DevOps team must predict and make provisions for expansion needs, including purchasing additional servers, expanding network infrastructure and increasing storage. . This scaling process can be costly and time-consuming, not to mention that at the end of the event there will be a large amount of idle resources left unused.\nTherefore, the DevOps team decided to Migrate services to the Cloud environment to get the following benefits:\nScalable flexibility: EKS cloud allows you to scale resources as needed without having to invest in hardware and infrastructure. You can easily increase or decrease the number of cluster nodes to meet the workload. Flexible expansion helps you save costs and optimize resource usage. Simplified infrastructure management: EKS cloud eliminates the difficulty and work involved in managing Kubernetes infrastructure. AWS manages and maintains the entire infrastructure, including installing, configuring, scaling, and upgrading Kubernetes components. This helps reduce management burden and focus on application development. AWS service integration: EKS cloud integrates well with other AWS services and tools. You can use services such as Amazon EC2, Amazon RDS, Amazon S3, and Amazon DynamoDB to supplement your applications. This helps leverage database management, data storage and processing services, minimizing integration work and increasing system reliability. Flexible costs: Migrate from an On-premises environment to EKS cloud helps you switch from a pre-pay model (CAPEX) to a pay-as-you-use model (OPEX). Instead of having to invest in hardware and infrastructure, you only pay to use the service using a pay-as-you-go model. This helps you optimize costs and be flexible in application development and operations. This Workshop focuses on demoing Migrate a simple Micro-service phonebook application deployed on a Kubernetes cluster in an On-premises environment to an AWS Cloud environment, as well as building an EKS cluster architecture using Infrastrure as code tool is Terraform. The Workshop does not cover CI/CD, monitoring, logging and integrating other AWS services such as RDS, Cloudwatch, etc.\n"
},
{
	"uri": "//localhost:1313/2-architecture/",
	"title": "Architecture ",
	"tags": [],
	"description": "",
	"content": "Content Current architecture of the Kubernetes system on On-premises environment System architecture when deployed to AWS EKS cluster "
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/",
	"title": "Build Infrastucture on AWS Cloud environment using Terraform",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s start with terraform. First, we need to create an AWS provider. It allows to interact with the many resources supported by AWS, such as VPC, EC2, EKS, and many others. You must configure the provider with the proper credentials before using it.\nIn this case, I use Access Key \u0026amp; Secret Key to configure credentials for Terraform\nprovider \u0026quot;aws\u0026quot; { region = \u0026quot;us-east-1\u0026quot; }\rterraform {\rrequired_providers {\raws = {\rsource = \u0026quot;hashicorp/aws\u0026quot;\rversion = \u0026quot;~\u0026gt; 3.0\u0026quot;\r}\r}\r}\rContent: Create AWS VPC \u0026amp; Subnets using Terraform Create Internet Gateway in AWS using Terraform Create NAT Gateway in AWS using Terraform Create Route Table in AWS using Terraform Create EKS cluster using Terraform Create IAM OIDC provider EKS using Terraform Access to EKS cluster "
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/3.1.2-create-access-secret-key/",
	"title": "Create access key &amp; secret key",
	"tags": [],
	"description": "",
	"content": "Step 1: Go to the AWS management console, click on your Profile name, and then click on Security Credentials Step 2: Go to Access Keys and select Create New Access Key. Step 3: Click on Show Access Key and save/download the access key and secret access key. "
},
{
	"uri": "//localhost:1313/3-migrate/3.3-k8s-app/3.3.2-alb-ingress-manifest/",
	"title": "Create ALB Ingress manifest file",
	"tags": [],
	"description": "",
	"content": "Create Security Group for AWS ALB using Terraform\nresource \u0026quot;aws_security_group\u0026quot; \u0026quot;alb-security-group\u0026quot; {\rname = \u0026quot;alb-security-group\u0026quot;\rdescription = \u0026quot;Allow all inbound traffic and all outbound traffic\u0026quot;\rvpc_id = aws_vpc.vpc-main.id\ringress {\rfrom_port = 0\rto_port = 0\rprotocol = \u0026quot;-1\u0026quot;\rcidr_blocks = [\u0026quot;0.0.0.0/0\u0026quot;]\rdescription = \u0026quot;Allow all traffic from VPC main\u0026quot;\r} egress {\rfrom_port = 0\rto_port = 0\rprotocol = \u0026quot;-1\u0026quot;\rcidr_blocks = [\u0026quot;0.0.0.0/0\u0026quot;]\rdescription = \u0026quot;Allow all traffic outbound to VPC main\u0026quot;\r}\r}\rThen run command terraform apply. The ID of the security group will appear.\nWe will use this ID to assign the Ingress rule\nNext, we need to create namepace for App-phonebook by command:\nkubectl create ns phonebook\rCreate manifest file to define Ingress rule for web-server \u0026amp; result-server\nSpecify the properties of the AWS ALB\nFor web-server:\napiVersion: networking.k8s.io/v1\rkind: Ingress\rmetadata:\rname: web-server\rnamespace: phonebook\rannotations:\r# Ingress Core Settings\rkubernetes.io/ingress.class: \u0026quot;alb\u0026quot;\ralb.ingress.kubernetes.io/scheme: internet-facing\ralb.ingress.kubernetes.io/target-type: ip\ralb.ingress.kubernetes.io/tags: app=phonebook\ralb.ingress.kubernetes.io/subnets: subnet-030ff556cb14be7d9, subnet-09b6fd037c314a022\r# Health Check Settings\ralb.ingress.kubernetes.io/healthcheck-protocol: HTTP alb.ingress.kubernetes.io/healthcheck-port: traffic-port\ralb.ingress.kubernetes.io/healthcheck-path: /\ralb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'\ralb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'\ralb.ingress.kubernetes.io/success-codes: '200'\ralb.ingress.kubernetes.io/healthy-threshold-count: '2'\ralb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\ralb.ingress.kubernetes.io/security-groups: sg-0e21986c4e1c60ef8\rspec:\ringressClassName: \u0026quot;alb\u0026quot;\rrules:\r- http:\rpaths:\r- path: /\rpathType: Prefix\rbackend:\rservice:\rname: phonebook-service\rport:\rnumber: 80\rFor result-server\napiVersion: networking.k8s.io/v1\rkind: Ingress\rmetadata:\rname: result-server\rnamespace: phonebook\rannotations:\r# Ingress Core Settings\rkubernetes.io/ingress.class: \u0026quot;alb\u0026quot;\ralb.ingress.kubernetes.io/scheme: internet-facing\ralb.ingress.kubernetes.io/target-type: ip\ralb.ingress.kubernetes.io/tags: app=phonebook\ralb.ingress.kubernetes.io/subnets: subnet-030ff556cb14be7d9, subnet-09b6fd037c314a022\r# Health Check Settings\ralb.ingress.kubernetes.io/healthcheck-protocol: HTTP alb.ingress.kubernetes.io/healthcheck-port: traffic-port\ralb.ingress.kubernetes.io/healthcheck-path: /\ralb.ingress.kubernetes.io/healthcheck-interval-seconds: '15'\ralb.ingress.kubernetes.io/healthcheck-timeout-seconds: '5'\ralb.ingress.kubernetes.io/success-codes: '200'\ralb.ingress.kubernetes.io/healthy-threshold-count: '2'\ralb.ingress.kubernetes.io/unhealthy-threshold-count: '2'\ralb.ingress.kubernetes.io/security-groups: sg-0e21986c4e1c60ef8\rspec:\ringressClassName: alb\rrules:\r- http:\rpaths:\r- path: /\rpathType: Prefix\rbackend:\rservice:\rname: result-service\rport:\rnumber: 80 "
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/3.2.2-igw/",
	"title": "Create Internet Gateway in AWS using Terraform",
	"tags": [],
	"description": "",
	"content": "To provide internet access for my services. Need to create internet gateway in the VPC, It will be used as a default route in public subnets\nresource \u0026quot;aws_internet_gateway\u0026quot; \u0026quot;internet-gateway-vpc-main\u0026quot; {\rvpc_id = aws_vpc.vpc-main.id\rtags = {\rName = \u0026quot;internet-gateway-vpc-main\u0026quot;\r}\r}\r"
},
{
	"uri": "//localhost:1313/4-backup-restore/4.2-push/",
	"title": "Push mysql backup file from On-premsise enviroment to AWS cloud enviroment using awscli",
	"tags": [],
	"description": "",
	"content": "To be able to push objects to S3 bucket, we need to install awscli and need confiure credential (Refer 3.1.4)\nWe have successfully uploaded, next we will restore the data on the AWS Cloud environment\n"
},
{
	"uri": "//localhost:1313/2-architecture/2.2-cloud/",
	"title": "System architecture when deployed to AWS EKS cluster",
	"tags": [],
	"description": "",
	"content": "\nThis is Infrastructure on the AWS cloud environment when migrated from On-premises. On the AWS Cloud environment, AWS is responsible for managing the Control Plane instead of us having to manage the Control Plane ourselves, ensuring High Availability and backup \u0026amp; restore scenarios when incidents occur on the On-premises environment. Reduces the burden of Infrastructure management on us and helps us focus on application development.\nOn the AWS Cloud environment, users will access the Phonebook Service using a domain that is resolved using AWS Route53. Then the request will go to the AWS Load balancer which is managed by the AWS ALB Ingress Controller instead of going through the Nginx Ingress Controller on the On-premises environment, then the request will go directly to the Pods that are deployed on the Workers node The DevOps team will manage the AWS EKS cluster by performing actions on the Bastion Host. The DevOps team will access the Bastion host via AWS Instance Connect\nWe will conduct Migrate services from On-premises environment to AWS cloud environment according to the following steps:\nFirst, we will create a Bastion host and install the necessary tools to perform apply Terraform code and manage AWS EKS cluster.\nNext, we will build Infrastructure on the AWS Cloud environment (Using the Infrastructure as code tool Terraform)\nNext, we will deploy the architecture of the Phonebook App using Kubernetes platform on the EKS cluster similar to the architecture of the Phonebook App on the On-premises environment (At this time, the company\u0026rsquo;s services have been deployed to the EKS cluster, but no data yet)\nFinally, we will deploy backup and restore App data on On-premises environment to EKS cluster\n"
},
{
	"uri": "//localhost:1313/2-architecture/2.1-on-prem/2.1.2-k8s-app/",
	"title": "The application architecture is deployed using Kubernetes on On-premises environment",
	"tags": [],
	"description": "",
	"content": "\nThe phonebook application architecture includes 3 modules (decouple services):\nDatabase Module: Store data in MySQL database, deployed using StatefulSet, data is stored in static volume web-service module: Responsible for implementing add/update/detele phonebook APIs that interact with the Database module, deployed using Deployment and auto scaling using HPA (Horizontal Pod Autoscaling) result-service module: Responsible for implementing the phonebook search APIs to interact with the Dataabse module, similar to the web-service module Users access through the domain, Nginx Ingress Controller is responsible for directing requests according to Ingress rules, requests will be redirected to Serivce and from Service redirected to Pod.\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/3.2.3-nat/",
	"title": "Create NAT Gateway in AWS using Terraform",
	"tags": [],
	"description": "",
	"content": "NAT Gateway is used in private subnets to allow services to connect to the internet. For NAT, need to allocate public IP address first.\nresource \u0026quot;aws_eip\u0026quot; \u0026quot;eip-vpc-main\u0026quot; {\rvpc = true\rtags = {\rName = \u0026quot;eip-vpc-main\u0026quot;\r}\r}\rresource \u0026quot;aws_nat_gateway\u0026quot; \u0026quot;nat-gateway-vpc-main\u0026quot; {\rallocation_id = aws_eip.eip-vpc-main.id\rsubnet_id = aws_subnet.public-subnet-vpc-main-1.id\rconnectivity_type = \u0026quot;public\u0026quot;\rtags = {\rName = \u0026quot;nat-gateway-vpc-main\u0026quot;\r}\r# Internet Gateway must be created before aws_nat_gateway\rdepends_on = [aws_internet_gateway.internet-gateway-vpc-main]\r}\r"
},
{
	"uri": "//localhost:1313/3-migrate/3.3-k8s-app/",
	"title": "Deploy App-phonebook architecture using Kubernetes platform on AWS EKS cluster on AWS cloud environment",
	"tags": [],
	"description": "",
	"content": "Content: AWS ALB Ingress Controller Create ALB Ingress manifest file Module web server Module result server Module MySQL database "
},
{
	"uri": "//localhost:1313/3-migrate/",
	"title": "Migrate service from On-premises environment to AWS cloud environment",
	"tags": [],
	"description": "",
	"content": "Content 3.1. Create Bastion Host and install the necessary tools 3.2. Build Infrastructure on AWS Cloud environment using Terraform 3.3. Deploy App-phonebook architecture using Kubernetes platform on AWS EKS cluster on AWS cloud environment\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.3-k8s-app/3.3.3-web/",
	"title": "Module web server",
	"tags": [],
	"description": "",
	"content": "Create Configmap, secret\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: servers-configmap\rnamespace: phonebook\rdata:\rMYSQL_DATABASE_HOST: mysql-service.phonebook.svc.cluster.local\rMYSQL_USER: devenes\rMYSQL_DATABASE: phonebook\r---\rapiVersion: v1\rkind: Secret\rmetadata:\rname: mysql-secret\rnamespace: phonebook\rtype: Opaque\rdata:\rmysql-root-password: SjEyMzQ1ag==\r# value: J12345j\rmysql-admin-password: RGV2ZW5lc18x\r# value: Devenes_1\r---\rapiVersion: v1\rkind: ConfigMap\rmetadata:\rname: database-configmap\rnamespace: phonebook\rdata:\rMYSQL_USER: devenes\rMYSQL_DATABASE: phonebook\rCreate Configmap for server: Specify the database to which the server connects\napiVersion: v1\rkind: ConfigMap\rmetadata:\rname: servers-configmap\rnamespace: phonebook\rdata:\rMYSQL_DATABASE_HOST: mysql-service.phonebook.svc.cluster.local\rMYSQL_USER: devenes\rMYSQL_DATABASE: phonebook\rCreate Deployment \u0026amp; Service type “ClusterIP” for web-server\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: phonebook-app-deploy\rnamespace: phonebook\rlabels:\rapp: phonebook-app-deploy\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rname: phonebook-app-pod\rtemplate:\rmetadata:\rname: phonebook-app-pod\rlabels:\rname: phonebook-app-pod\rspec:\rcontainers:\r- name: phonebook-app\rresources:\rlimits:\rcpu: 200m\rmemory: 400Mi\rrequests:\rcpu: 200m\rmemory: 400Mi\rimage: devenes/python_web_server:2\rports:\r- containerPort: 80\renv:\r- name: MYSQL_PASSWORD\rvalueFrom:\rsecretKeyRef:\rname: mysql-secret\rkey: mysql-admin-password\renvFrom:\r- configMapRef:\rname: servers-configmap\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: phonebook-service\rnamespace: phonebook\rlabels:\rname: phonebook-service\rspec:\rtype: ClusterIP\rports:\r- port: 80\rtargetPort: 80\rselector:\rname: phonebook-app-pod\rCreate HorizontalPodAutoscaler to auto scale web-server Pod, It perform action CPU scaling to operate at 50% of the CPU required by the Pod. (Make sure Metric Server is installed)\napiVersion: autoscaling/v1\rkind: HorizontalPodAutoscaler\rmetadata:\rname: hpa-deploy-web-server\rnamespace: phonebook\rspec:\rscaleTargetRef:\rapiVersion: apps/v1\rkind: Deployment\rname: phonebook-app-deploy\rminReplicas: 1\rmaxReplicas: 10\rtargetCPUUtilizationPercentage: 50\r"
},
{
	"uri": "//localhost:1313/4-backup-restore/4.3-restore/",
	"title": "Restore data for mysql database on AWS cloud enviroment",
	"tags": [],
	"description": "",
	"content": "Similar to the On-premises environment, we pull the file from the S3 bucket and copy it to the /mnt/data folder on the Host. Then access MySQL Pod and restore with command:\nmysql -u root -p phonebook \u0026lt; database_ phonebook.sql After the restore process is complete, we check the data again in the Cloud environment:\nWe have seen that the data has been restored successfully.\nSo we have successfully migrated services deployed using Kubernetes from On-premises environment to Cloud environment.\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/3.1.3-instance-connect/",
	"title": "Setup Instance Connect to access Bastion host via ssh protocol",
	"tags": [],
	"description": "",
	"content": "Amazon EC2 Instance Connect is a simple and secure way to connect to your instances using Secure Shell (SSH). With EC2 Instance Connect, you can control SSH access to your instances using AWS Identity and Access Management (IAM) policies as well as audit connection requests with AWS CloudTrail events.\nFor AWS Instance Connect to be able to access Bastion Host, the Bastion Host must be located on the public network and the Security Group must have an Inbound rule opened for the refer IP range: https://ip-ranges.amazonaws.com/ip-ranges.json To connect to your instance using the browser-based client from the Amazon EC2 console:\nStep 1: Select the instance and choose Connect. Step 2: Click to Connect: We have successfully accessed Bastion Host using AWS EC2 Instance Connect: From here we will start installing the necessary tools\n"
},
{
	"uri": "//localhost:1313/4-backup-restore/",
	"title": "Backup &amp; restore application data from on-premises enviroment to AWS cloud enviroment",
	"tags": [],
	"description": "",
	"content": "Depending on whether the application\u0026rsquo;s data is large or small and requires a short or long time to upload data, we will choose different backup \u0026amp; restore scenarios.\nWith this application, we will use mysqldump to backup the phonebook database on the On-premises environment, then push it to the S3 bucket so that the Server on the AWS Cloud environment can pull it back. Finally, restore on the mysql database deployed on AWS Cloud\nContent: Backup mysql database on On-premsise enviroment Push mysql backup file from On-premsise enviroment to AWS cloud enviroment using awscli Restore data for mysql database on AWS cloud enviroment "
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/3.2.4-routetable/",
	"title": "Create Route Table in AWS using Terraform",
	"tags": [],
	"description": "",
	"content": "A route table contains a set of rules, called routes, that determine where network traffic from your subnet or gateway is directed\nRoute Table for Public Subnets\nresource \u0026quot;aws_route_table\u0026quot; \u0026quot;route-table-public-vpc-main\u0026quot; {\rvpc_id = aws_vpc.vpc-main.id\rroute {\rcidr_block = \u0026quot;0.0.0.0/0\u0026quot;\rgateway_id = aws_internet_gateway.internet-gateway-vpc-main.id\r}\rtags = {\rName = \u0026quot;route-table-public-vpc-main\u0026quot;\r}\r}\rresource \u0026quot;aws_route_table_association\u0026quot; \u0026quot;public-subnet-1\u0026quot; {\rsubnet_id = aws_subnet.public-subnet-vpc-main-1.id\rroute_table_id = aws_route_table.route-table-public-vpc-main.id\r}\rresource \u0026quot;aws_route_table_association\u0026quot; \u0026quot;public-subnet-2\u0026quot; {\rsubnet_id = aws_subnet.public-subnet-vpc-main-2.id\rroute_table_id = aws_route_table.route-table-public-vpc-main.id\r}\rRoute Table for Private Subnets\nresource \u0026quot;aws_route_table\u0026quot; \u0026quot;route-table-private-vpc-main\u0026quot; {\rvpc_id = aws_vpc.vpc-main.id\rroute {\rcidr_block = \u0026quot;0.0.0.0/0\u0026quot;\rnat_gateway_id = aws_nat_gateway.nat-gateway-vpc-main.id\r}\rtags = {\rName = \u0026quot;route-table-private-vpc-main\u0026quot;\r}\r}\rresource \u0026quot;aws_route_table_association\u0026quot; \u0026quot;private-subnet-1\u0026quot; {\rsubnet_id = aws_subnet.private-subnet-vpc-main-1.id\rroute_table_id = aws_route_table.route-table-private-vpc-main.id\r}\rresource \u0026quot;aws_route_table_association\u0026quot; \u0026quot;private-subnet-2\u0026quot; {\rsubnet_id = aws_subnet.private-subnet-vpc-main-2.id\rroute_table_id = aws_route_table.route-table-private-vpc-main.id\r} "
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/3.1.4-awscli/",
	"title": "Install awscli tool",
	"tags": [],
	"description": "",
	"content": "Step 1: Update the local package index and to provide the system with the most recent information about available packages from the repositories\nsudo apt update -y Step 2: Download awscli binary file\ncurl \u0026quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026quot; -o \u0026quot;awscliv2.zip\u0026quot;\rStep 3: unzip then install\nunzip awscliv2.zip\rsudo ./aws/install\rStep 4: Configure for AWS CLI\naws configure\raws_access_key_id = xxx\raws_secret_access_key = xxx\rregion = xxx\routput = xxx\r"
},
{
	"uri": "//localhost:1313/3-migrate/3.3-k8s-app/3.3.4-result/",
	"title": "Module result server",
	"tags": [],
	"description": "",
	"content": "Create Deployment \u0026amp; Service type “ClusterIP” for result-server\napiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: result-app-deploy\rnamespace: phonebook\rlabels:\rname: result-app-deploy\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rname: result-app-pod\rtemplate:\rmetadata:\rname: result-app-pod\rlabels:\rname: result-app-pod\rspec:\rcontainers:\r- name: result-app\rresources:\rimage: devenes/python_result_server:2\rports:\r- containerPort: 80\renv:\r- name: MYSQL_PASSWORD\rvalueFrom:\rsecretKeyRef:\rname: mysql-secret\rkey: mysql-admin-password\renvFrom:\r- configMapRef:\rname: servers-configmap\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: result-service\rnamespace: phonebook\rlabels:\rname: result-service\rspec:\rtype: ClusterIP\rports:\r- port: 80\rtargetPort: 80\rselector:\rname: result-app-pod\rCreate HorizontalPodAutoscaler to auto scale result-server Pod, It perform action CPU scaling to operate at 50% of the CPU required by the Pod. (Make sure Metric Server is installed)\napiVersion: autoscaling/v1\rkind: HorizontalPodAutoscaler\rmetadata:\rname: hpa-deploy-result-server\rnamespace: phonebook\rspec:\rscaleTargetRef:\rapiVersion: apps/v1\rkind: Deployment\rname: result-app-deploy\rminReplicas: 1\rmaxReplicas: 10\rtargetCPUUtilizationPercentage: 50\r"
},
{
	"uri": "//localhost:1313/5-clear/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "Because all resources, Phonebook applications, and application data are deployed on the AWS Cloud environment, after completing this Workshop, if we want to clean up resources, we just need to run command: terraform destroy and enter yes. Then the resource cleaning process will take place automatically:\nThe process will continue until the resource created by Terraform is completely destroyed\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/3.2.5-eks-cluster/",
	"title": "Create EKS cluster using Terraform",
	"tags": [],
	"description": "",
	"content": "Kubernetes clusters managed by Amazon EKS make calls to other AWS services on your behalf to manage the resources that we use with the service. For example, EKS will create an autoscaling group for each instance group if you use managed nodes.\nBefore creating Amazon EKS clusters, we must create an IAM role with the AmazonEKSClusterPolicy.\nresource \u0026quot;aws_iam_role\u0026quot; \u0026quot;eks-cluster-role\u0026quot; {\rname = \u0026quot;eks-cluster-role\u0026quot;\rassume_role_policy = \u0026lt;\u0026lt;POLICY\r{\r\u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;,\r\u0026quot;Statement\u0026quot;: [\r{\r\u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;,\r\u0026quot;Principal\u0026quot;: {\r\u0026quot;Service\u0026quot;: \u0026quot;eks.amazonaws.com\u0026quot;\r},\r\u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot;\r}\r]\r}\rPOLICY\r}\rresource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;eks-cluster-role-attach-AmazonEKSClusterPolicy\u0026quot; {\rpolicy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\u0026quot;\rrole = aws_iam_role.eks-cluster-role.name\r}\rresource \u0026quot;aws_eks_cluster\u0026quot; \u0026quot;phonebook-eks-cluster\u0026quot; {\rname = \u0026quot;phonebook-eks-cluster\u0026quot;\rrole_arn = aws_iam_role.eks-cluster-role.arn\rvpc_config {\rsubnet_ids = [\raws_subnet.private-subnet-vpc-main-1.id,\raws_subnet.private-subnet-vpc-main-2.id,\raws_subnet.public-subnet-vpc-main-1.id,\raws_subnet.public-subnet-vpc-main-2.id\r]\r}\rtags = {\rApp = \u0026quot;phonebook\u0026quot;\r}\rdepends_on = [aws_iam_role_policy_attachment.eks-cluster-role-attach-AmazonEKSClusterPolicy]\r}\rNext, we will create a Node Group for EKS cluster. However, before creating a Node Group we need to attach the necessary policies to its Role\nresource \u0026quot;aws_iam_role\u0026quot; \u0026quot;node-group-role\u0026quot; {\rname = \u0026quot;node-group-role\u0026quot;\rassume_role_policy = jsonencode({\rStatement = [{\rAction = \u0026quot;sts:AssumeRole\u0026quot;\rEffect = \u0026quot;Allow\u0026quot;\rPrincipal = {\rService = \u0026quot;ec2.amazonaws.com\u0026quot;\r}\r}]\rVersion = \u0026quot;2012-10-17\u0026quot;\r})\r}\rresource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;node-group-role-attach-AmazonEKSWorkerNodePolicy\u0026quot; {\rpolicy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\u0026quot;\rrole = aws_iam_role.node-group-role.name\r}\rresource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;node-group-role-attach-AmazonEKS_CNI_Policy\u0026quot; {\rpolicy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\u0026quot;\rrole = aws_iam_role.node-group-role.name\r}\rresource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;node-group-role-attach-AmazonEC2ContainerRegistryReadOnly\u0026quot; {\rpolicy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\u0026quot;\rrole = aws_iam_role.node-group-role.name\r}\rresource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;node-group-role-attach-AmazonS3FullAccess\u0026quot; {\rpolicy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonS3FullAccess\u0026quot;\rrole = aws_iam_role.node-group-role.name\r}\rresource \u0026quot;aws_iam_role_policy_attachment\u0026quot; \u0026quot;node-group-role-attach-AmazonEC2FullAccess\u0026quot; {\rpolicy_arn = \u0026quot;arn:aws:iam::aws:policy/AmazonEC2FullAccess\u0026quot;\rrole = aws_iam_role.node-group-role.name\r}\rNext, we write Terraform code to create Node Group with: - Name of Node Group is \u0026ldquo;phonebook-nodes\u0026rdquo; into \u0026ldquo;phonebook-eks-cluster\u0026rdquo; EKS cluster - Network is private-subnet-vpc-main-1 - ami is Amazon Linux 2 x86_64, capacity type is on demand, instance type is t2.medium and storage disk is 10Gib - scaling config: desired = 1, max = 1 \u0026amp; min = 1\nresource \u0026quot;aws_eks_node_group\u0026quot; \u0026quot;phonebook-nodes\u0026quot; {\rcluster_name = aws_eks_cluster.phonebook-eks-cluster.name\rnode_group_name = \u0026quot;phonebook-nodes\u0026quot;\rnode_role_arn = aws_iam_role.node-group-role.arn\rsubnet_ids = [\raws_subnet.private-subnet-vpc-main-1.id\r]\r# Node group compute configuration\rami_type = \u0026quot;AL2_x86_64\u0026quot;\rcapacity_type = \u0026quot;ON_DEMAND\u0026quot;\rinstance_types = [\u0026quot;t2.medium\u0026quot;]\rdisk_size = 10\rscaling_config {\rdesired_size = 1\rmax_size = 1\rmin_size = 1\r}\r# Node group update configuration\rupdate_config {\rmax_unavailable = 1\r}\rlabels = {\rapp = \u0026quot;phonebook\u0026quot;\r}\rdepends_on = [\raws_iam_role_policy_attachment.node-group-role-attach-AmazonEKSWorkerNodePolicy,\raws_iam_role_policy_attachment.node-group-role-attach-AmazonEKS_CNI_Policy,\raws_iam_role_policy_attachment.node-group-role-attach-AmazonEC2ContainerRegistryReadOnly,\raws_iam_role_policy_attachment.node-group-role-attach-AmazonS3FullAccess,\raws_iam_role_policy_attachment.node-group-role-attach-AmazonEC2FullAccess\r]\r} "
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/3.1.5-eksctl/",
	"title": "Install eksctl tool",
	"tags": [],
	"description": "",
	"content": "Step 1: Specify environment variables for ARM systems, set ARCH to: arm64, armv6 or armv7\nARCH=amd64\rPLATFORM=$(uname -s)_$ARCH\rStep 2: Download eksctl-install file\ncurl -sLO \u0026quot;https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz\u0026quot;\r(Optional) Verify checksum\rcurl -sL \u0026quot;https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt\u0026quot; | grep $PLATFORM | sha256sum --check\rStep 3: De-compression then move to folder bin\ntar -xzf eksctl_$PLATFORM.tar.gz -C /tmp \u0026amp;\u0026amp; rm eksctl_$PLATFORM.tar.gz\rsudo mv /tmp/eksctl /usr/local/bin\r"
},
{
	"uri": "//localhost:1313/3-migrate/3.3-k8s-app/3.3.5-mysql/",
	"title": "Module MySQL database",
	"tags": [],
	"description": "",
	"content": "Create Static Volume: Create PersistentVolumeClaim bound to PersistentVolume\napiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: mysql-pv-volume\rnamespace: phonebook\rlabels:\rtype: local\rspec:\rstorageClassName: manual\rcapacity:\rstorage: 2Gi\raccessModes:\r- ReadWriteOnce\rhostPath:\rpath: \u0026quot;/mnt/data\u0026quot;\r---\rapiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: mysql-pv-claim\rnamespace: phonebook\rspec:\rstorageClassName: manual\raccessModes:\r- ReadWriteOnce\rresources:\rrequests:\rstorage: 2Gi\rCreate StatefulSet \u0026amp; Service type “ClusterIP” for mysql database: Specify static volume and configmap and secret just created above\napiVersion: apps/v1\rkind: StatefulSet\rmetadata:\rname: mysql-deploy\rnamespace: phonebook\rlabels:\rname: mysql-deploy\rspec:\rreplicas: 1\rserviceName: mysql-service\rselector:\rmatchLabels:\rname: mysql-pod\rtemplate:\rmetadata:\rname: mysql-pod\rlabels:\rname: mysql-pod\rspec: containers:\r- image: mysql:5.7\rresources:\rname: mysql\rimagePullPolicy: Always\renv:\r- name: MYSQL_PASSWORD\rvalueFrom:\rsecretKeyRef:\rname: mysql-secret\rkey: mysql-admin-password\r- name: MYSQL_ROOT_PASSWORD\rvalueFrom:\rsecretKeyRef:\rname: mysql-secret\rkey: mysql-root-password\renvFrom:\r- configMapRef:\rname: database-configmap\rports:\r- containerPort: 3306\rvolumeMounts:\r- name: mysql-persistent-storage\rmountPath: /var/lib/mysql\rvolumes:\r- name: mysql-persistent-storage\rpersistentVolumeClaim:\rclaimName: mysql-pv-claim\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: mysql-service\rnamespace: phonebook\rlabels:\rname: mysql-service\rspec:\rports:\r- port: 3306\rtargetPort: 3306\rselector:\rname: mysql-pod\rWe have prepared the manifest files defining the architecture of the Phonebook App on the AWS Cloud environment. To deploy, we will run command:\nkubectl apply --f app-phonebook/. Where app-phonebook is the folder containing manifest files. We have successfully deployed Phonebook App on EKS Cluster:\nApplication interface:\nAdd/Update/Delete phone book APIs:\nResult phone book APIs:\nAt this time, the company\u0026rsquo;s services have been deployed to the EKS cluster, but no data yet. We need to backup and restore App data on On-premises environment to EKS cluster\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/3.2.6-oidc/",
	"title": "Create IAM OIDC provider EKS using Terraform",
	"tags": [],
	"description": "",
	"content": "To manage permissions for the applications that deployed in Kubernetes. We can either attach policies to Kubernetes nodes directly. In that case, every pod will get the same access to AWS resources. Or we can create OpenID connect provider, which will allow granting IAM permissions based on the service account used by the pod.\nOpenID Connect is an authentication protocol which allows to verify user identity when a user is trying to access a protected HTTPs end point. OIDC is an evolutionary development of ideas implemented earlier in OAuth and OpenID.\ndata \u0026quot;tls_certificate\u0026quot; \u0026quot;eks\u0026quot; {\rurl = aws_eks_cluster.phonebook-eks-cluster.identity[0].oidc[0].issuer }\rresource \u0026quot;aws_iam_openid_connect_provider\u0026quot; \u0026quot;eks\u0026quot; {\rclient_id_list = [\u0026quot;sts.amazonaws.com\u0026quot;]\rthumbprint_list = [data.tls_certificate.eks.certificates[0].sha1_fingerprint]\rurl = aws_eks_cluster.phonebook-eks-cluster.identity[0].oidc[0].issuer\r}\rWe complete to create main.tf file, then we will run command to run Terraform code\nterraform init: initializes a working directory and downloads the necessary provider plugins and modules and setting up the backend for storing your infrastructure\u0026rsquo;s state\nterraform plan: creates a dry-run, determining what actions are necessary to achieve the desired state defined in the Terraform configuration files\nterraform apply: to deploy in AWS cloud\nEnter yes, then AWS resources begin to be created\nWe have completed to create Infrastructure in AWS cloud by Terraform (Infrastructure as code tool)\n"
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/3.1.6-kubectl/",
	"title": "Install kubectl tool",
	"tags": [],
	"description": "",
	"content": "Step 1: Download binary file\ncurl -LO https://dl.k8s.io/release/v1.28.4/bin/linux/amd64/kubectl\rStep 2: Install kubectl\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\rStep 3: Command check version of kubectl\nkubectl version --client\r"
},
{
	"uri": "//localhost:1313/3-migrate/3.2-terraform/3.2.7-access-eks-cluster/",
	"title": "Access to EKS cluster",
	"tags": [],
	"description": "",
	"content": "To access the EKS cluster, you need to install aws-iam-authenticator on the bastion host, then update kubeconfig AWS IAM Authenticator for Kubernetes is a command-line tool that helps manage cluster access authentication for Amazon Elastic Container Service for Kubernetes (EKS)\nBy leveraging IAM Authenticator, users can authenticate against their AWS IAM credentials and seamlessly utilize the powerful capabilities of EKS. This simplifies the authentication process and helps maintain a secure and manageable environment for Kubernetes clusters.\n# Download binary file\rcurl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator\r# Add EXECUTE permission\rchmod +x ./aws-iam-authenticator\r# Move binary file to folder bin\rsudo mv ./aws-iam-authenticator /usr/local/bin\rNext, update kubeconfig by command:\naws eks update-kubeconfig --name phonebook-eks-cluster\rAfter running command above, kubeconfig file was updated in part /root/.kube/config. We can use kubectl tool to manage EKS cluster "
},
{
	"uri": "//localhost:1313/3-migrate/3.1-setup-tools/3.1.7-terraform/",
	"title": "Install Terraform tool",
	"tags": [],
	"description": "",
	"content": "Refer: https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli\nStep 1: Ensure that your system is up to date and you have installed:\ninstalled the gnupg software-properties-common curl packages You will use these packages to verify HashiCorp\u0026rsquo;s GPG signature and install HashiCorp\u0026rsquo;s Debian package repository.\nsudo apt-get update -y \u0026amp;\u0026amp; sudo apt-get install -y gnupg software-properties-common\rStep 2: Install the HashiCorp GPG key.\nwget -O- https://apt.releases.hashicorp.com/gpg | \\\rgpg --dearmor | \\\rsudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg \u0026gt; /dev/null\rStep 3: Verify the key\u0026rsquo;s fingerprint.\ngpg --no-default-keyring \\\r--keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg \\\r--fingerprint\rThe gpg command will report the key fingerprint:\r/usr/share/keyrings/hashicorp-archive-keyring.gpg\r-------------------------------------------------\rpub rsa4096 XXXX-XX-XX [SC]\rAAAA AAAA AAAA AAAA\ruid [ unknown] HashiCorp Security (HashiCorp Package Signing) \u0026lt;security+packaging@hashicorp.com\u0026gt;\rsub rsa4096 XXXX-XX-XX [E]\rStep 4: Add the official HashiCorp repository to your system.\nThe lsb_release -cs command finds the distribution release codename for your current system, such as buster, groovy, or sid.\necho \u0026quot;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \\\rhttps://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026quot; | \\\rsudo tee /etc/apt/sources.list.d/hashicorp.list\rStep 5: Download the package information from HashiCorp.\nsudo apt update -y\rStep 6: Install Terraform from the new repository.\nsudo apt-get install terraform -y\r*** Verify the installation\nterraform --help\r*** Add any subcommand to terraform -help to learn more about what it does and available options.\nterraform --help plan\r"
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]